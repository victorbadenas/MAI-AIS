Before going into details of all the different implementation and applications of SHAP, it's important give a good explanation about the most important concepts needed to know for understanding why the algorithm works and why it has quickly become one of the most used Explainable Artificial Intelligence method.
\section{Additive Feature Attribution Methods}
The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand.\\
Instead, we must use a \textit{simpler explanation model}, that use simplified inputs $x'$ that map to the original inputs through a mapping function $x = h_{x}(x')$, which we define as any interpretable approximation of the original model.

All additive feature attribution methods have an explanation model that is a linear function of binary variables:
$$g(z')=\phi_0+\sum_{j=1}^M\phi_jz_j'$$

Methods with explanation models matching the previous expression (that will be explained later in \autoref{definition}) attribute an effect $\phi_i$ to each feature, and summing the effects of all feature attributions approximates the output f(x) of the original model \cite{Interpreting-Model-Predictions}.

In other words, we can say that feature attribution refers to the fact that the change of an outcome to be explained (e.g., a class probability in a classification problem) with respect to a baseline (e.g., average prediction probability for that class in the training set) can be attributed in different proportions to the model input features.


\section{Shapley Values}

The Shapley value is a solution concept in cooperative game theory, named after Lloyd Shapley. To each cooperative game it assigns a unique distribution (among the players) of a total surplus generated by the coalition of players.

For example: a group of players cooperates and obtains a gain from the cooperation. The contribution of the players to the overall gain is not usually uniform, some contribute more than others. The players that contribute more hold more bargaining power (for example threatening to destroy the whole surplus). The Shapley value answers the question of how important is each player to the overall cooperation, and what payoff can he or she expect?

In Machine Learning models, Shapley values can be defined analogously to the above description. In this case, the game would be the prediction task for the instance, the players are the features and finally the gain is the prediction for this instance minus the average prediction for all instances. Shapley values allows us to numerically describe the contribution of the features to an output of a model.

\subsection{Nomenclature}

\begin{enumerate}
    \item A set of $N$ players
    \item A function $v$ that maps subsets of players (simplified features) to real numbers (gains)  $v : 2^N \rightarrow \mathbb {R}$, where $v(\emptyset) = 0$.
    \item A coalition of players noted $S$ for which $v(S)$ is the worth of the coalition of players and $v(x),\ x=\{p_1, ..., p_n\}\ where\ p_i \in S$ describes the expected payoff for the group of players $x$.
    \item $\phi(v)$ denotes the shapley value for the the gain function $v$
\end{enumerate}

\subsection{Computation example}

We start from a model trained to predict apartment prices based on 2 features: `park` (nearby, far), `area` (numerical), `floor` (cardinal/numerical) and `cat` (allowed, banned). The average prediction for all apartments is 310.000€. Let's compute the coalition for the combination `park-nearby`, `cat-banned` and `area-50` and let's simulate that the `floor` variable is not in the coalition. In order to do that we have to perform two steps:
\begin{itemize}
    \item To simulate the absence of the `floor` feature, a random instance of the dataset is extracted and the `floor` attribute is used to predict the first value f.i. 310.000€.
    \item The `cat-banned` is removed from the coalition and replaced with a random value of the cat allowed or banned and predict the price f.i. 320.000€.
    \item Compute the difference between the two values.
\end{itemize}

The goal is to explain the difference between the instance prediction and the average prediction. For example, to evaluate the contribution of the `cat-banned` in a coalition with `park-nearby`, `area-50` and `floor-2nd`. Given a set of simplified features (each feature is either included or excluded), all combinations of feature coalitions are evaluated with and without the cat-banned feature and the difference is computed. After all the combinations are computed, the Shapley value is the weighted average of all the marginal contributions. The feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model.

\subsection{Properties}

\begin{enumerate}
    \item \textbf{Efficiency}: The sum of the Shapley values of all players equals the value of the grand coalition, so that all the gain is distributed among the agents.
    \item \textbf{Symmetry}: If $i$ and $j$ are two players who are equivalent in the sense that the addition of each player to a set contributes to the same gain $v(S \cup \{i\}) = v(S \cup \{j\})$, then $\phi_i(v) = \phi_j(v)$ their Shapley values are the same.
    \item \textbf{Linearity}: Shapley values have to satisfy: \begin{itemize}
        \item $\phi_i(v+w) = \phi_i(v) + \phi_i(w)$
        \item $\phi_i(av) = a*\phi_i(v)$
    \end{itemize}
    \item \textbf{Null player}: If the gain is the same for a player group independent if another player is in the group, the Shapley value for that player is 0. If $v(S\cup\{i\}) = v(S)$ then $\phi_i(v) = 0$
    \item \textbf{Anonymity}: The labeling of the players doesn't affect the assignation of their gain
    \item \textbf{Marginalism}: The Shapley value can be defined as a function which uses only the marginal contributions of player i as the arguments.
\end{enumerate}

\subsection{Advantages}
The main advantages of the Shapley values are:
\begin{enumerate}
    \item The difference between the prediction and the average prediction is \textbf{fairly distributed}. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features. The Shapley value might be the only method to deliver a full explanation. In situations where the law requires explainability the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly.
    \item The Shapley value allows \textbf{contrastive explanations}. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have.
    \item The Shapley value is the only explanation method with a \textbf{solid theory}. Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work.
\end{enumerate}

\subsection{Disadvantages}
The main disadvantages of Shapley values are:
\begin{enumerate}
    \item The Shapley value requires \textbf{a lot of computing time}. In 99.9\% of real-world problems, only the approximate solution is feasible. An exact computation of the Shapley value is computationally expensive because there are $2^k$  possible coalitions of the feature values and the "absence" of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation. The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but increases the variance of the Shapley value. There is no good rule of thumb for the number of iterations M. M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.
    \item The Shapley value \textbf{can be misinterpreted}. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.
    \item The Shapley value returns a simple value per feature, but \textbf{no prediction model} like LIME. This means it cannot be used to make statements about changes in prediction for changes in the input, such as: "If I were to earn €300 more a year, my credit score would increase by 5 points."
    \item Like many other permutation-based interpretation methods, the Shapley value method suffers from \textbf{inclusion of unrealistic data} instances when features are correlated. To simulate that a feature value is missing from a coalition, we marginalize the feature.
\end{enumerate}

\subsection{Estimation}
To estimate a Shapley value, all the sets of features have to be evaluated with and without the feature to extract the Shapley value. The computation of the exact solution grows exponentially with the number of features so usually a Monte-Carlo sampling is used:
\begin{equation}\label{eqn:estimation_formula}
\hat{\phi}_{j}=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}(x^{m}_{+j})-\hat{f}(x^{m}_{-j})\right)
\end{equation}



where $\hat{f}(x^{m}_{+j})$ is the prediction for x, but with a random number of feature values replaced by feature values from a random data point z, except for the respective value of feature j.
The x-vector $x^{m}_{-j}$ is almost identical to $x^{m}_{+j}$, but the value $x_j^{m}$ is also taken from the sampled z.

\subsection{Single Shapley value estimation}

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Shapley value for the value of the j-th feature}
 initialization\;
 \textbf{Required: }Number of iterations M, instance of interest x, feature index j, data matrix X, and machine learning model f\linebreak\linebreak
 \For{m=1,...,M:}{
    Draw random instance z from the data matrix X\;
    Choose a random permutation o of the feature values\;
    Order instance x: $x_o=(x_{(1)},\ldots,x_{(j)},\ldots,x_{(p)})$\;
    Order instance z: $z_o=(z_{(1)},\ldots,z_{(j)},\ldots,z_{(p)})$\;
    Construct two new instances: \begin{itemize}
        \item With feature j: $x_{+j}=(x_{(1)},\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\ldots,z_{(p)})$
        \item Without feature j: $x_{-j}=(x_{(1)},\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\ldots,z_{(p)})$
    \end{itemize}
    Compute marginal contribution: $\phi_j^{m}=\hat{f}(x_{+j})-\hat{f}(x_{-j})$\;
 }
 Compute Shapley value as the average: $\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$
 \caption{}
\end{algorithm}
%good explanation of shap and the difference with LIME usable for the presentation too:

%https://elula.ai/how-common-explainable-ai-algorithms-work/

%https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30

%to a better understanding of the plots
%https://towardsdatascience.com/how-to-avoid-the-machine-learning-blackbox-with-shap-da567fc64a8b
